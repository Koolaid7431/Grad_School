{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Activities Recognition - 2020\n",
    "\n",
    "### By: Kashif Siddiqui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import create_model as cmodel\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "fs = 20 # sampling frequency\n",
    "\n",
    "activity_names = {}\n",
    "activity_names['A'] = 'Walking'\n",
    "activity_names['B'] = 'Jogging'\n",
    "activity_names['C'] = 'Stairs'\n",
    "activity_names['D'] = 'Sitting'\n",
    "activity_names['E'] = 'Standing'\n",
    "activity_names['F'] = 'Typing'\n",
    "activity_names['G'] = 'Brushing Teeth'\n",
    "activity_names['H'] = 'Eating Soup'\n",
    "activity_names['I'] = 'Eating Chips'\n",
    "activity_names['J'] = 'Eating Pasta'\n",
    "activity_names['K'] = 'Drinking from Cups'\n",
    "activity_names['L'] = 'Eating Sandwich'\n",
    "activity_names['M'] = 'Kicking (Soccer Ball)'\n",
    "activity_names['O'] = 'Playing Catch w/Tennis Ball'\n",
    "activity_names['P'] = 'Dribbling Basketball'\n",
    "activity_names['Q'] = 'Writing'\n",
    "activity_names['R'] = 'Clapping'\n",
    "activity_names['S'] = 'Folding Clothes'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(activity_names[chr(65)])\n",
    "test= list(activity_names)\n",
    "print(test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_person_activity(input_directory, file_name):\n",
    "    full_name = input_directory + '/' + file_name\n",
    "    activity_dic = {}\n",
    "    for indx in range(ord('A'),ord('S')+1): # Label for activities are in range 'A' to 'S' (except 'N')\n",
    "        activity_dic[(chr(indx))] = []\n",
    "    activity_dic.pop('N') # There is NOT label 'N' in the dataset\n",
    "\n",
    "    with open(full_name,'r') as f:\n",
    "        sample = f.readlines()\n",
    "    sample_len = len(sample)\n",
    "\n",
    "    for i in range(sample_len): \n",
    "        tmp = sample[i].split(',') # e.g. tmp looks like: '1600,A,252207666810782,-0.36476135,8.793503,1.0550842;\\n'\n",
    "        activity_dic[tmp[1]].append([tmp[3], tmp[4], tmp[5].split(';')[0]])\n",
    "    subject_code = tmp[0]\n",
    "        \n",
    "    return subject_code, activity_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_signal(subj_activities, action, activity_names):\n",
    "    activity = np.asarray(subj_activities[action],dtype=np.float64)\n",
    "    #activity = np.asarray(subj_activities[action])\n",
    "    x = activity[0:,[0]]\n",
    "    y = activity[0:,[1]]\n",
    "    z = activity[0:,[2]]\n",
    "    data = subj_activities[action]\n",
    "    label2 = activity_names[action]\n",
    "    #plot3(x,y,z,fs, activity_names[action]+'_0-end seconds')\n",
    "    return x,y,z, data, label2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "66\n",
      "67\n",
      "82\n"
     ]
    }
   ],
   "source": [
    "for i in [65,66,67,82]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3574, 1)\n",
      "-0.36476135\n"
     ]
    }
   ],
   "source": [
    "input_directory = '/dataset/HAR/wisdm-dataset/raw/phone/accel'\n",
    "file_name = 'data_1600_accel_phone.txt'\n",
    "subj_code, subj_activities = read_person_activity(input_directory, file_name)\n",
    "x,y,z,data,label = full_signal(subj_activities, chr(65), activity_names)\n",
    "print(x.shape)\n",
    "\n",
    "#print(subj_activities)\n",
    "\n",
    "\n",
    "val = subj_activities.values\n",
    "temp = list(subj_activities.values())\n",
    "print(temp[0][0][0])\n",
    "#arr = np.array([[1, 2, 3, 7], [4, 5, 6, 8]])\n",
    "#arr= np.array([x,y,z])\n",
    "#train_data = []\n",
    "#train_data.append(arr)\n",
    "#train_data.append(arr)\n",
    "#train_data = np.asarray(train_data,dtype=np.float64)\n",
    "#print(train_data)\n",
    "#print(train_data.shape)\n",
    "#print(len(train_data[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data2D(input_directory):\n",
    "    '''\n",
    "    Input Arguements:\n",
    "    directory = Full path to the folder containing dataset, containing the .mat and .hea files\n",
    "    Output:\n",
    "    returns numpy arrays of train_data,train_labels,val_data,val_labels\n",
    "    The code from Calvin creats the filenames in the order of their name, but here I used listdir for the list of files\n",
    "    '''\n",
    "    input_files = []\n",
    "    for f in os.listdir(input_directory):\n",
    "        if os.path.isfile(os.path.join(input_directory, f)):\n",
    "            input_files.append(f)\n",
    "    #random.shuffle(input_files)\n",
    "    num_files = len(input_files)\n",
    "    print('total number of files: ', num_files)\n",
    "    #classes = get_classes(input_directory,input_files)\n",
    "\n",
    "    # Create training/val/test split (80/10/10)\n",
    "\n",
    "    train_split = int(num_files*0.8) \n",
    "    val_split = int(num_files*0.1)\n",
    "    test_split = int(num_files*0.1)\n",
    "    print('Number of files for trainig: ', train_split)\n",
    "    print('Number of files for validation: ', val_split)\n",
    "    print('Number of files for testing: ', test_split)\n",
    "\n",
    "    # Lists to hold data and labels\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    val_data = []\n",
    "    val_labels = []\n",
    "\n",
    "    \n",
    "    \n",
    "    # Create Training Data and Labels\n",
    "    for file_number in range(train_split):\n",
    "        if (file_number+1)%500 == 0:\n",
    "            print('{} files were loaded for training!'.format(file_number+1))\n",
    "\n",
    "        # Load file for data\n",
    "        subject_code, activity_dic = read_person_activity(input_directory, input_files[file_number])\n",
    "\n",
    "        # This loop read all 12 leads one by one and put them together into \"window\" list, then adds \"window\" to the train_data\n",
    "        for i in [65,66,67,68,69,70,71,72,73,74,75,76,77,79,80,81,82,83]: #no 78 in the list for \"N\"\n",
    "            \n",
    "            x,y,z, data, label = full_signal(activity_dic, chr(i), activity_names)\n",
    "            data_app= np.array([x,y,z])\n",
    "            \n",
    "            train_data.append(data_app)\n",
    "\n",
    "            train_labels.append(chr(i))\n",
    "                \n",
    "                \n",
    "    # Repeat the above code for validation data\n",
    "    for file_number in range(train_split,num_files - test_split):\n",
    "        if (file_number+1)%500 == 0:\n",
    "            print('{} files were loaded for validation!'.format(file_number+1-train_split))\n",
    "\n",
    "        # Load file for data\n",
    "        subject_code, activity_dic = read_person_activity(input_directory, input_files[file_number])\n",
    "\n",
    "        # This loop read all 12 leads one by one and put them together into \"window\" list, then adds \"window\" to the train_data\n",
    "        for i in [65,66,67,68,69,70,71,72,73,74,75,76,77,79,80,81,82,83]: #no 78 in the list for \"N\"\n",
    "            \n",
    "            x,y,z, data, label = full_signal(activity_dic, chr(i), activity_names)\n",
    "            data_app= np.array([x,y,z])\n",
    "            val_data.append(data_app)\n",
    "\n",
    "            val_labels.append(chr(i))\n",
    "\n",
    "    c = 0\n",
    "    with open('HAR_test_files_00.txt', 'w') as f:\n",
    "        for file_number in range(train_split+val_split,num_files):\n",
    "            c += 1\n",
    "            f.write(input_files[file_number]+ '\\n')\n",
    "    print('number of test signals = ', c) \n",
    "\n",
    "\n",
    "    print('train_data type:',  type(train_data))\n",
    "    print('train_labels  type:',  type(train_labels))\n",
    "    print('val_data  type:',  type(val_data))\n",
    "    print('val_labels  type:', type(val_labels))\n",
    "\n",
    "\n",
    "    print('train_data length', len(train_data))\n",
    "    print('train_labels length', len(train_labels))\n",
    "    print('val_data length:', len(val_data))\n",
    "    print('val_labels length:', len(val_labels))\n",
    "\n",
    "    \n",
    "    # Convert to numpy array since TF likes those better than lists\n",
    "    train_data = np.asarray(train_data,dtype=np.float64)\n",
    "    train_labels = np.asarray(train_labels)\n",
    "    val_data = np.asarray(val_data,dtype=np.float64)\n",
    "    val_labels = np.asarray(val_labels)\n",
    "\n",
    "    print('train_data type:',  type(train_data))\n",
    "    print('train_labels  type:',  type(train_labels))\n",
    "    print('val_data  type:',  type(val_data))\n",
    "    print('val_labels  type:', type(val_labels))\n",
    "\n",
    "    print('train_data.shape:', train_data.shape)\n",
    "    print('train_labels.shape:', train_labels.shape)\n",
    "    print('val_data.shape:', val_data.shape)\n",
    "    print('val_labels.shape:', val_labels.shape)\n",
    "    \n",
    "    # reshape train and validation data to 2D for 2D CNN\n",
    "    dim = train_data.shape\n",
    "    train_data = train_data.reshape(dim[0],dim[1],dim[2],1)\n",
    "    dim = val_data.shape\n",
    "    val_data = val_data.reshape(dim[0],dim[1],dim[2],1)\n",
    "    \n",
    "    print('train_data.shape:', train_data.shape)\n",
    "    print('val_data.shape:', val_data.shape)\n",
    "\n",
    "    return(train_data,train_labels,val_data,val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tf(train_data,train_labels,val_data,val_labels):\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_labels))\n",
    "\n",
    "    BATCH_SIZE = 64 # Training batch size \n",
    "    SHUFFLE_BUFFER_SIZE = 512 # Determines randomness? Need to doublecheck\n",
    "\n",
    "    # Create training and validation batches\n",
    "\n",
    "    train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "    print('train_dataset type: ', type(train_dataset))\n",
    "    print('val_dataset type: ', type(val_dataset))\n",
    "    return train_dataset,val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of files:  52\n",
      "Number of files for trainig:  41\n",
      "Number of files for validation:  5\n",
      "Number of files for testing:  5\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-16705cf661d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-16705cf661d6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0minput_directory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/dataset/HAR/wisdm-dataset/raw/phone/accel'\u001b[0m \u001b[0;31m# Dataset on beastie\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_tf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-09f19da23795>\u001b[0m in \u001b[0;36mload_data2D\u001b[0;34m(input_directory)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m65\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m66\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m67\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m68\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m69\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m71\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m72\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m73\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m74\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m76\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m77\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m79\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m81\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m82\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m83\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#no 78 in the list for \"N\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivity_dic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivity_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mdata_app\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-5c4d10d4d116>\u001b[0m in \u001b[0;36mfull_signal\u001b[0;34m(subj_activities, action, activity_names)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mactivity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubj_activities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#activity = np.asarray(subj_activities[action])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\tt1 = time()\n",
    "\tinput_directory = '/dataset/HAR/wisdm-dataset/raw/phone/accel' # Dataset on beastie \n",
    "\ttrain_data,train_labels,val_data,val_labels = load_data2D(input_directory) \n",
    "\ttrain_dataset,val_dataset = load_tf(train_data,train_labels,val_data,val_labels)\n",
    "\tt2 = time()\n",
    "\tprint('Loading data in {} seconds.'.format(round(t2-t1)))\n",
    "\tt1 = time()    \n",
    "\tmodel = cmodel.create_model2D() # Change model type if necessary\n",
    "\ta,l,vl,va = cmodel.train_model(model,train_dataset,val_dataset,model_name=\"HARv01_00\",show_stats = False) # Change name or show plots directly if desired\n",
    "\tt2 = time()\n",
    "\tprint('Training DeepCNN in {} seconds.'.format(round(t2-t1)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating_Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device:/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "#import seaborn as sns\n",
    "#import warnings\n",
    "#import csv\n",
    "import gc\n",
    "from time import time\n",
    "import logging\n",
    "#from scipy.io import loadmat\n",
    "#from select_win import select_win,select_windows\n",
    "import load_data as ldfile\n",
    "#import create_model as cmodel\n",
    "import glob\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "tf.device('/device:GPU:0')\n",
    "\n",
    "#from scipy.signal import butter, sosfilt, sosfilt_zi, sosfiltfilt, lfilter, lfilter_zi, filtfilt, sosfreqz, resample\n",
    "#from utils import hamilton_detector, christov_detector, findpeaks, engzee_detector\n",
    "from ecg_detectors.ecgdetectors import Detectors, MWA, panPeakDetect, searchBack\n",
    "\n",
    "np.random.seed(354)\n",
    "#sns.set()\n",
    "#warnings.filterwarnings('ignore')\n",
    "#logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "tf.random.set_seed(1342) # Set seed for reproducibility\n",
    "\n",
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "   print(\"Please install GPU version of TF\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Up Images from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of files:  6877\n",
      "Number of files for trainig:  5501\n",
      "Number of files for validation:  687\n",
      "Number of files for testing:  687\n",
      "500 files were loaded for training!\n",
      "1000 files were loaded for training!\n",
      "1500 files were loaded for training!\n",
      "2000 files were loaded for training!\n",
      "2500 files were loaded for training!\n",
      "3000 files were loaded for training!\n",
      "3500 files were loaded for training!\n",
      "4000 files were loaded for training!\n",
      "4500 files were loaded for training!\n",
      "5000 files were loaded for training!\n",
      "5500 files were loaded for training!\n",
      "499 files were loaded for validation!\n",
      "Labels:\n",
      "0. 164884008\n",
      "1. 164889003\n",
      "2. 164909002\n",
      "3. 164931005\n",
      "4. 270492004\n",
      "5. 284470004\n",
      "6. 426783006\n",
      "7. 429622005\n",
      "8. 59118001\n",
      "\n",
      "number of test signals =  689\n",
      "\n",
      "train_data type: <class 'list'>\n",
      "train_labels  type: <class 'list'>\n",
      "val_data  type: <class 'list'>\n",
      "val_labels  type: <class 'list'>\n",
      "\n",
      "train_data length 26828\n",
      "train_labels length 26828\n",
      "val_data length: 3532\n",
      "val_labels length: 3532\n",
      "\n",
      "train_data type: <class 'numpy.ndarray'>\n",
      "train_labels_bin  type: <class 'numpy.ndarray'>\n",
      "val_data  type: <class 'numpy.ndarray'>\n",
      "val_labels_bin  type: <class 'numpy.ndarray'>\n",
      "\n",
      "train_data.shape: (26828, 12, 2500)\n",
      "train_labels.shape: (26828, 9)\n",
      "val_data.shape: (3532, 12, 2500)\n",
      "val_labels.shape: (3532, 9)\n",
      "\n",
      "train_data.shape: (26828, 12, 2500, 1)\n",
      "val_data.shape: (3532, 12, 2500, 1)\n",
      "Loading data in 102 seconds.\n"
     ]
    }
   ],
   "source": [
    "t1 = time()\n",
    "input_directory = '/dataset/PhysioNet2020/Training_WFDB' # Dataset on beastie \n",
    "train_data,train_labels_bin,val_data,val_labels_bin = ldfile.load_data2D_bin(input_directory) \n",
    "#train_dataset,val_dataset = ldfile.load_tf(train_data,train_labels_bin,val_data,val_labels_bin)\n",
    "t2 = time()\n",
    "print('Loading data in {} seconds.'.format(round(t2-t1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26828, 12, 2500, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Data from the dataset as an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plot(data1, i, dir_path):\n",
    "    \n",
    "    #plt.figure()\n",
    "    #figure(num=None, figsize=(100,10), dpi = 100)\n",
    "    \n",
    "    #fig,lead=plt.subplots(12,1, figsize=(20,60)) #### ORIGINAL - FOR ALL 12 LEADS\n",
    "    fig,lead=plt.subplots(4,1, figsize=(10,20), dpi = 100)\n",
    "    #plt.gca().set_axis_off()\n",
    "    \n",
    "    plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, hspace = 0, wspace = 0) #deals with all the whitespace around the plots\n",
    "    #plt.margins(0,0)\n",
    "\n",
    "    lead[0].plot(data1[0],'black')\n",
    "    lead[0].set_axis_off()\n",
    "    #lead[1].plot(data1[1],'black')\n",
    "    #lead[1].set_axis_off()\n",
    "    #lead[2].plot(data1[2],'black')\n",
    "    #lead[2].set_axis_off()\n",
    "    #lead[3].plot(data1[3],'black')\n",
    "    #lead[3].set_axis_off()\n",
    "    lead[1].plot(data1[4],'black')\n",
    "    lead[1].set_axis_off()\n",
    "    #lead[5].plot(data1[5],'black')\n",
    "    #lead[5].set_axis_off()\n",
    "    lead[2].plot(data1[6],'black')\n",
    "    lead[2].set_axis_off()\n",
    "    #lead[7].plot(data1[7],'black')\n",
    "    #lead[7].set_axis_off()\n",
    "    #lead[8].plot(data1[8],'black')\n",
    "    #lead[8].set_axis_off()\n",
    "    #lead[9].plot(data1[9],'black')\n",
    "    #lead[9].set_axis_off()\n",
    "    #lead[10].plot(data1[10],'black')\n",
    "    #lead[10].set_axis_off()\n",
    "    lead[3].plot(data1[11],'black')\n",
    "    lead[3].set_axis_off()\n",
    "    \n",
    "    file_name = 'img%d.png' %(i) \n",
    "    #fig.tight_layout()\n",
    "    #plt.show()\n",
    "    fig.savefig(os.path.join(dir_path,file_name), transparent = None, bbox_inches = 'tight', pad_inches = 0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEPS:\n",
    "\n",
    "1. Create an output folder (test to see if one exists)\n",
    "2. Loop through 'train_data' -> plot images (do only 5000 at a time)\n",
    "\n",
    "3. Save image into output folder\n",
    "4. Take plotted image, open it and read it with imread (as an array item) \n",
    "5. append it to an array 'train_im'\n",
    "\n",
    "6. Read through 'train_labels_bin' and copy section (5000 at a time) into 'train_lab'\n",
    "\n",
    "7. Repeat process for 'val_data' and 'val_labels_bin'\n",
    "\n",
    "8. Make train_dataset and val_dataset from data and labels combined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ERROR** - Separate 'train_batch' into 2 functions and break the loop for automating into two parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch (train_data, start, end):\n",
    "    \n",
    "    # test block to test the function in step 1:\n",
    "    dir_path = \"train_outputs_v1\"\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    diff = end-start\n",
    "    range_end = 1000+diff\n",
    "    # test block to test the loop in step 2 & 3:\n",
    "    for i,j in zip (range(start, end), range(10000,range_end)):  #the second range is for the file numbers you're gonna generate in the \n",
    "        test_1 = create_plot(train_data[i], j, dir_path)\n",
    "    \n",
    "\n",
    "    # test blcok to test saving the img in step 4 & 5:\n",
    "    X_data = []\n",
    "    files = sorted(glob.glob('train_outputs_v1/*.png')) #change temp_outputs to your temporary storage area for images created\n",
    "    for myFile in files:\n",
    "        image = cv2.imread (myFile)\n",
    "        X_data.append (image)\n",
    "    #train_im = np.asarray(X_data)\n",
    "    \n",
    "    #return train_im\n",
    "    return X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round: 0 start: 0 end: 1000\n",
      "converting into array for round: 0\n",
      "round: 1 start: 1000 end: 2000\n",
      "converting into array for round: 1\n",
      "round: 2 start: 2000 end: 3000\n",
      "converting into array for round: 2\n",
      "round: 3 start: 3000 end: 4000\n",
      "converting into array for round: 3\n",
      "round: 4 start: 4000 end: 5000\n",
      "converting into array for round: 4\n",
      "round: 5 start: 5000 end: 6000\n",
      "converting into array for round: 5\n",
      "(0,)\n",
      "Loading data in 1 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Tracking for Training Data:\n",
    "\n",
    "#Train_data size: 26828\n",
    "#single 'train_batch': 500\n",
    "#number of batches to load: 53.656\n",
    "\n",
    "t1 = time()\n",
    "\n",
    "#train_d1 = [[]]\n",
    "for i in range(0,6):\n",
    "    length = 1000 # pick how big do you want your length to be\n",
    "    start = i*length\n",
    "    end = start+length\n",
    "    print('round:', i, 'start:', start, 'end:', end)\n",
    "    t_d = train_batch(train_data, start, end)\n",
    "    print('converting into array for round:', i)\n",
    "    if i==0:\n",
    "        train_d = np.asarray(t_d)\n",
    "        #print('starting to work')\n",
    "    else:\n",
    "        train_d1 = np.asarray(t_d)\n",
    "        #print('is it working?')\n",
    "        train_d = np.append(train_d, train_d1 , axis=0) \n",
    "    \n",
    "    del t_d\n",
    "    gc.collect()\n",
    "\n",
    "print(train_d.shape)\n",
    "    \n",
    "np.save('train_v1.npy', train_d)\n",
    "del train_d, train_d1\n",
    "gc.collect()\n",
    "    \n",
    "t2 = time()\n",
    "print('Loading data in {} seconds.'.format(round(t2-t1)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT DO THIS UNTIL ALL 26828  are loaded in train_set_#.npy files\n",
    "#del train_data\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VALIDATION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_batch (val_data, start, end):\n",
    "    \n",
    "\n",
    "    # test block to test the function in step 1:\n",
    "    dir_path = \"val_outputs_v1\"\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    # test block to test the loop in step 2 & 3:\n",
    "    diff = end-start\n",
    "    range_end = 1000+diff\n",
    "    for i,j in zip (range(0,1000), range(10000,range_end)): #the second range is for the file numbers you're gonna generate in the \n",
    "        test_1 = create_plot(val_data[i], j, dir_path)\n",
    "\n",
    "    # test blcok to test saving the img in step 4 & 5:\n",
    "    X_data = []\n",
    "    files = sorted(glob.glob('val_outputs_v1/*.png')) #change temp_outputs to your temporary storage area for images created\n",
    "    for myFile in files:\n",
    "        \n",
    "        image = cv2.imread (myFile)\n",
    "        X_data.append (image)\n",
    "    #val_im = np.asarray(X_data)\n",
    "\n",
    "    \n",
    "    #return val_im\n",
    "    return X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round: 0 start: 0 end: 100\n",
      "converting into array for round: 0\n",
      "round: 1 start: 100 end: 200\n",
      "converting into array for round: 1\n",
      "Loading data in 150 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Tracking for Validation Data:\n",
    "\n",
    "#Train_data size: 3532\n",
    "#single 'train_batch': 500\n",
    "#number of batches to load: ~6\n",
    "\n",
    "t1 = time()\n",
    "\n",
    "#train_d1 = [[]]\n",
    "for i in range(0,3):\n",
    "    length = 1000 # pick how big do you want your length to be\n",
    "    start = i*length\n",
    "    end = start+length\n",
    "    print('round:', i, 'start:', start, 'end:', end)\n",
    "    v_d = val_batch(val_data, start, end)\n",
    "    print('converting into array for round:', i)\n",
    "    if i==0:\n",
    "        val_d = np.asarray(v_d)\n",
    "    else:\n",
    "        val_d1 = np.asarray(v_d)\n",
    "        val_d = np.append(val_d, val_d1 , axis=0) \n",
    "    \n",
    "    del v_d\n",
    "    gc.collect()\n",
    "    \n",
    "print(val_d.shape)\n",
    "    \n",
    "np.save('val_v1.npy', val_d)\n",
    "del val_d, val_d1\n",
    "gc.collect()\n",
    "    \n",
    "t2 = time()\n",
    "print('Loading data in {} seconds.'.format(round(t2-t1)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT DO THIS UNTIL ALL 26828  are loaded in train_set_#.npy files\n",
    "#del val_data\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

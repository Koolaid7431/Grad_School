{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import tkinter as tk\n",
    "\n",
    "import sys\n",
    "if sys.version_info >= (3,0):\n",
    "    from queue import Queue\n",
    "else:\n",
    "    from Queue import Queue\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import scipy.misc as misc\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from scipy.misc import imresize\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "\n",
    "    #########################################################################\n",
    "    # Game configuration\n",
    "\n",
    "    # Name of the game, with version (e.g. PongDeterministic-v0)\n",
    "    ATARI_GAME = 'PongDeterministic-v0'\n",
    "\n",
    "    # Enable to see the trained agent in action\n",
    "    PLAY_MODE = False\n",
    "\n",
    "    # Input of the DNN\n",
    "    STACKED_FRAMES = 4\n",
    "    IMAGE_WIDTH = 84\n",
    "    IMAGE_HEIGHT = 84\n",
    "    \n",
    "MODE = 'actor'\n",
    "    \n",
    "FIRST_FRAME = 350\n",
    "NUM_FRAMES = 100\n",
    "\n",
    "DENSITY = 5\n",
    "RADIUS = 5\n",
    "FUDGE_FACTOR = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameManager:\n",
    "    def __init__(self, game_name, display):\n",
    "        self.game_name = game_name\n",
    "        self.display = display\n",
    "\n",
    "        self.env = gym.make(game_name)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        return observation\n",
    "\n",
    "    def step(self, action):\n",
    "        self._update_display()\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def _update_display(self):\n",
    "        if self.display:\n",
    "            self.env.render()\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.game = GameManager(Config.ATARI_GAME, display=Config.PLAY_MODE)\n",
    "        self.nb_frames = Config.STACKED_FRAMES\n",
    "        self.frame_q = Queue(maxsize=self.nb_frames)\n",
    "        self.previous_state = None\n",
    "        self.current_state = None\n",
    "        self.total_reward = 0\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    @staticmethod\n",
    "    def _rgb2gray(rgb):\n",
    "        return np.dot(rgb[..., :3], [0.299, 0.587, 0.114])\n",
    "\n",
    "    @staticmethod\n",
    "    def _preprocess(image):\n",
    "        image = Environment._rgb2gray(image)\n",
    "        image = misc.imresize(image, [Config.IMAGE_HEIGHT, Config.IMAGE_WIDTH], 'bilinear')\n",
    "        image = image.astype(np.float32) / 128.0 - 1.0\n",
    "        return image\n",
    "\n",
    "    def _get_current_state(self):\n",
    "        if not self.frame_q.full():\n",
    "            return None  # frame queue is not full yet.\n",
    "        x_ = np.array(self.frame_q.queue)\n",
    "        x_ = np.transpose(x_, [1, 2, 0])  # move channels\n",
    "        return x_\n",
    "\n",
    "    def _update_frame_q(self, frame):\n",
    "        if self.frame_q.full():\n",
    "            self.frame_q.get()\n",
    "        image = Environment._preprocess(frame)\n",
    "        self.frame_q.put(image)\n",
    "\n",
    "    def get_num_actions(self):\n",
    "        return self.game.env.action_space.n\n",
    "\n",
    "    def reset(self):\n",
    "        self.total_reward = 0\n",
    "        self.frame_q.queue.clear()\n",
    "        self._update_frame_q(self.game.reset())\n",
    "        self.previous_state = self.current_state = None\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, done, _ = self.game.step(action)\n",
    "\n",
    "        self.total_reward += reward\n",
    "        self._update_frame_q(observation)\n",
    "\n",
    "        self.previous_state = self.current_state\n",
    "        self.current_state = self._get_current_state()\n",
    "        return reward, done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, device, model_name, num_actions):\n",
    "        self.device = device \n",
    "        self.model_name = model_name\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        self.img_width = Config.IMAGE_WIDTH\n",
    "        self.img_height = Config.IMAGE_HEIGHT\n",
    "        self.img_channels = Config.STACKED_FRAMES\n",
    "\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default() as g:\n",
    "            with tf.device(self.device):\n",
    "                self.create_placeholder()\n",
    "                self.create_network()\n",
    "                # self.create_train_op()\n",
    "                self.sess = tf.Session(\n",
    "                    graph=self.graph,\n",
    "                    config=tf.ConfigProto(\n",
    "                        allow_soft_placement=True,\n",
    "                        log_device_placement=False,\n",
    "                        gpu_options=tf.GPUOptions(allow_growth=True)))\n",
    "                self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "                vars = tf.trainable_variables()\n",
    "                self.saver = tf.train.Saver({var.name: var for var in vars}, max_to_keep=0)\n",
    "\n",
    "    def create_placeholder(self):\n",
    "        self.x = tf.placeholder(\n",
    "            tf.float32, [None, self.img_height, self.img_width, self.img_channels], name='X')\n",
    "\n",
    "    def create_network(self):\n",
    "        # As implemented in A3C paper\n",
    "        self.n1 = self.conv2d_layer(self.x, 8, 16, 'conv11', strides=[1, 4, 4, 1])\n",
    "        self.n2 = self.conv2d_layer(self.n1, 4, 32, 'conv12', strides=[1, 2, 2, 1])\n",
    "        self.action_index = tf.placeholder(tf.float32, [None, self.num_actions])\n",
    "        _input = self.n2\n",
    "\n",
    "        flatten_input_shape = _input.get_shape()\n",
    "        nb_elements = flatten_input_shape[1] * flatten_input_shape[2] * flatten_input_shape[3]\n",
    "\n",
    "        self.flat = tf.reshape(_input, shape=[-1, nb_elements._value])\n",
    "        self.d1 = self.dense_layer(self.flat, 256, 'dense1')\n",
    "\n",
    "        self.logits_v = tf.squeeze(self.dense_layer(self.d1, 1, 'logits_v', func=None), axis=[1])\n",
    "        self.logits_p = self.dense_layer(self.d1, self.num_actions, 'logits_p', func=None)\n",
    "        self.softmax_p = tf.nn.softmax(self.logits_p)\n",
    "\n",
    "    def dense_layer(self, input, out_dim, name, func=tf.nn.relu):\n",
    "        in_dim = input.get_shape().as_list()[-1]\n",
    "        d = 1.0 / np.sqrt(in_dim)\n",
    "        with tf.variable_scope(name):\n",
    "            w_init = tf.random_uniform_initializer(-d, d)\n",
    "            b_init = tf.random_uniform_initializer(-d, d)\n",
    "            w = tf.get_variable('w', dtype=tf.float32, shape=[in_dim, out_dim], initializer=w_init)\n",
    "            b = tf.get_variable('b', shape=[out_dim], initializer=b_init)\n",
    "\n",
    "            output = tf.matmul(input, w) + b\n",
    "            if func is not None:\n",
    "                output = func(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def conv2d_layer(self, input, filter_size, out_dim, name, strides, func=tf.nn.relu):\n",
    "        in_dim = input.get_shape().as_list()[-1]\n",
    "        d = 1.0 / np.sqrt(filter_size * filter_size * in_dim)\n",
    "        with tf.variable_scope(name):\n",
    "            w_init = tf.random_uniform_initializer(-d, d)\n",
    "            b_init = tf.random_uniform_initializer(-d, d)\n",
    "            w = tf.get_variable('w',\n",
    "                                shape=[filter_size, filter_size, in_dim, out_dim],\n",
    "                                dtype=tf.float32,\n",
    "                                initializer=w_init)\n",
    "            b = tf.get_variable('b', shape=[out_dim], initializer=b_init)\n",
    "\n",
    "            output = tf.nn.conv2d(input, w, strides=strides, padding='SAME') + b\n",
    "            if func is not None:\n",
    "                output = func(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def predict_p_and_v_single(self, x):\n",
    "        p, v = self.sess.run([self.softmax_p, self.logits_v], feed_dict={self.x: x[np.newaxis, :]})\n",
    "        return p[0], v[0]\n",
    "\n",
    "    def _checkpoint_filename(self, episode):\n",
    "        return 'checkpoints/%s_%08d' % (self.model_name, episode)\n",
    "\n",
    "    def _get_episode_from_filename(self, filename):\n",
    "        # TODO: hacky way of getting the episode. ideally episode should be stored as a TF variable\n",
    "        return int(re.split('/|_|\\.', filename)[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sailency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occlude(img, mask):\n",
    "    ret = np.zeros_like(img)\n",
    "    for d in range(img.shape[2]):\n",
    "        ret[:, :, d] = img[:, :, d] * (1 - mask) + gaussian_filter(img[:, :, d], sigma=3) * mask\n",
    "    return ret\n",
    "\n",
    "def get_mask(center, size, r):\n",
    "    y,x = np.ogrid[-center[0]:size[0]-center[0], -center[1]:size[1]-center[1]]\n",
    "    keep = x*x + y*y <= 1\n",
    "    mask = np.zeros(size) ; mask[keep] = 1 # select a circle of pixels\n",
    "    mask = gaussian_filter(mask, sigma=r) # blur the circle of pixels. this is a 2D Gaussian for r=r^2=1\n",
    "    return mask/mask.max()\n",
    "\n",
    "def score_frame(network, experiences, frame_id, radius, density, mode='actor'):\n",
    "    # with original state\n",
    "    if mode == 'actor':\n",
    "        L, _ = network.predict_p_and_v_single(experiences[frame_id].state)\n",
    "    elif mode == 'critic':\n",
    "        _, L = network.predict_p_and_v_single(experiences[frame_id].state)\n",
    "    scores = np.zeros((int(Config.IMAGE_HEIGHT / density) + 1, int(Config.IMAGE_WIDTH / density) + 1))\n",
    "    for i in range(0, Config.IMAGE_HEIGHT, density):\n",
    "        for j in range(0, Config.IMAGE_WIDTH, density):\n",
    "            mask = get_mask(center=[i,j], size=[Config.IMAGE_HEIGHT, Config.IMAGE_WIDTH], r=radius)\n",
    "            # with occluded state\n",
    "            if mode == 'actor':\n",
    "                l, _ = network.predict_p_and_v_single(occlude(experiences[frame_id].state, mask))\n",
    "            elif mode == 'critic':\n",
    "                _, l = network.predict_p_and_v_single(occlude(experiences[frame_id].state, mask))\n",
    "            scores[int(i / density), int(j / density)] = np.square(L - l).sum() * 0.5\n",
    "\n",
    "    pmax = scores.max()\n",
    "    scores = imresize(scores, size=[Config.IMAGE_HEIGHT, Config.IMAGE_WIDTH], interp='bilinear').astype(np.float32)\n",
    "    return pmax * scores / scores.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/pong/network_00029000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:41: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:32: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ 0 / 100 ] processing perturbation_map ... \n",
      " [ 1 / 100 ] processing perturbation_map ... \n",
      " [ 2 / 100 ] processing perturbation_map ... \n",
      " [ 3 / 100 ] processing perturbation_map ... \n",
      " [ 4 / 100 ] processing perturbation_map ... \n",
      " [ 5 / 100 ] processing perturbation_map ... \n",
      " [ 6 / 100 ] processing perturbation_map ... \n",
      " [ 7 / 100 ] processing perturbation_map ... \n",
      " [ 8 / 100 ] processing perturbation_map ... \n",
      " [ 9 / 100 ] processing perturbation_map ... \n",
      " [ 10 / 100 ] processing perturbation_map ... \n",
      " [ 11 / 100 ] processing perturbation_map ... \n",
      " [ 12 / 100 ] processing perturbation_map ... \n",
      " [ 13 / 100 ] processing perturbation_map ... \n",
      " [ 14 / 100 ] processing perturbation_map ... \n",
      " [ 15 / 100 ] processing perturbation_map ... \n",
      " [ 16 / 100 ] processing perturbation_map ... \n",
      " [ 17 / 100 ] processing perturbation_map ... \n",
      " [ 18 / 100 ] processing perturbation_map ... \n",
      " [ 19 / 100 ] processing perturbation_map ... \n",
      " [ 20 / 100 ] processing perturbation_map ... \n",
      " [ 21 / 100 ] processing perturbation_map ... \n",
      " [ 22 / 100 ] processing perturbation_map ... \n",
      " [ 23 / 100 ] processing perturbation_map ... \n",
      " [ 24 / 100 ] processing perturbation_map ... \n",
      " [ 25 / 100 ] processing perturbation_map ... \n",
      " [ 26 / 100 ] processing perturbation_map ... \n",
      " [ 27 / 100 ] processing perturbation_map ... \n",
      " [ 28 / 100 ] processing perturbation_map ... \n",
      " [ 29 / 100 ] processing perturbation_map ... \n"
     ]
    }
   ],
   "source": [
    "class Experience(object):\n",
    "    def __init__(self, state, action, prediction, reward, done):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.prediction = prediction\n",
    "        self.reward = reward\n",
    "        self.done = done\n",
    "\n",
    "env = Environment()\n",
    "network = Network(\"cpu:0\", \"network\", env.get_num_actions())\n",
    "if Config.ATARI_GAME == 'PongDeterministic-v0':\n",
    "    network.saver.restore(network.sess, './checkpoints/pong/network_00029000')\n",
    "elif Config.ATARI_GAME == 'BreakoutDeterministic-v0':\n",
    "    network.saver.restore(network.sess, './checkpoints/breakout/network_00097000')\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "env.reset()\n",
    "done = False\n",
    "experiences = []\n",
    "\n",
    "while not done:\n",
    "    # very first few frames \n",
    "    if env.current_state is None:\n",
    "        env.step(0) # 0 == NOOP\n",
    "        continue\n",
    "\n",
    "    prediction, value = network.predict_p_and_v_single(env.current_state)\n",
    "    action = np.argmax(prediction)\n",
    "    reward, done = env.step(action)\n",
    "    exp = Experience(env.previous_state, action, prediction, reward, done)\n",
    "    experiences.append(exp)\n",
    "\n",
    "frames = []\n",
    "perturbation_maps = []\n",
    "for frame_id in range(FIRST_FRAME, FIRST_FRAME + NUM_FRAMES):\n",
    "    sailency = score_frame(network, experiences, frame_id, RADIUS, DENSITY, mode=MODE)\n",
    "    pmax = sailency.max()\n",
    "\n",
    "    sailency -= sailency.min() ; sailency = FUDGE_FACTOR * pmax * sailency / sailency.max()\n",
    "    frames.append(experiences[frame_id].state[:, :, 3])\n",
    "    perturbation_maps.append(experiences[frame_id].state[:, :, 3] + sailency)\n",
    "    print(' [ %d / %d ] processing perturbation_map ... ' % (frame_id - FIRST_FRAME, NUM_FRAMES))\n",
    "\n",
    "# Visualize\n",
    "fig = plt.Figure()\n",
    "\n",
    "root = tk.Tk()\n",
    "\n",
    "label = tk.Label(root, text=\"Video\")\n",
    "label.grid(column=0, row=0)\n",
    "\n",
    "canvas = FigureCanvasTkAgg(fig, master=root)\n",
    "canvas.get_tk_widget().grid(column=0, row=1)\n",
    "\n",
    "ax_1 = fig.add_subplot(121)\n",
    "ax_2 = fig.add_subplot(122)\n",
    "\n",
    "\n",
    "def vedio(i):\n",
    "    frame = frames.pop(0)\n",
    "    frames.append(frame)\n",
    "    ax_1.clear()\n",
    "    ax_1.imshow(frame, vmin=0, vmax=1, cmap='gray')\n",
    "    p_map = perturbation_maps.pop(0)\n",
    "    perturbation_maps.append(p_map)\n",
    "    ax_2.clear()\n",
    "    ax_2.imshow(p_map, vmin=0, vmax=1, cmap='gray') #actor_sailency)\n",
    "\n",
    "ani = animation.FuncAnimation(fig, vedio, 1, interval=200)\n",
    "tk.mainloop()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

# -*- coding: utf-8 -*-
"""A1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cHwBf5rOCp-iSht61caf4VC4eBPgx8jp

# Assignment 1
### CHEM ENG/ SEP 787

### XuLiang Qi - 4000347697
### Mohammad Kashif Siddiqui - 0755452

### Headers
"""

import numpy as np
import matplotlib.pyplot as plt
import os 
import pandas as pd
from IPython.display import Image
from sklearn.metrics import mean_squared_error
import math
import random as rn
import time
from sklearn import datasets
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
import sklearn

"""### Checking paths"""

print('current path:')
dir_path = os.getcwd()
print(dir_path)

"""# Question 1
#### Using the comma separated multivariate data in the file fld.txt:

#### (a) Determine the discriminant line found by Fishers Linear Discriminant.

#### (b) Plot both the data and the discriminant line on a scatter plot

#### (c) Using this line, determine the class of each of the data points in the training dataset, assuming that the threshold is 0 (i.e. positive values are in one class and negative values in the other). 

#### (d) Determine what percentage of data points are incorrectly classifier

"""

# Reading the data
tic = time.clock()
data = pd.read_csv('fld.txt', header = None)
data = pd.DataFrame.to_numpy(data)
toc = time.clock()
print('Elapsed time in seconds:', (toc - tic))

# loading the data to an array and separating classes

x1=data[0:300,0:2]    # class = 1
x2=data[300:500, 0:2] # class = 0

test_labels = data[:,2]

# Calculate the mean for each group
u1 = np.mean(x1, axis = 0)
u2 = np.mean(x2, axis = 0)

# Remove the mean for each group
tic = time.clock()

x1ur = x1 - u1
x2ur = x2 - u2

# Calculate the Cov for each group
cov1 = np.dot(x1ur.T, x1ur)
cov2 = np.dot(x2ur.T, x2ur)

# Answer to Q1 a
# FLD using the method from class
sw = cov1 + cov2
w = np.dot(np.linalg.inv(sw), (u1 - u2))

toc = time.clock()
print('Elapsed time for inclass method in seconds:', (toc - tic))


# Plt
plt.figure()
plt.scatter(x1[:, 0], x1[:, 1], c='r')
plt.scatter(x2[:, 0], x2[:, 1], c='b')
plt.plot([-10000 * w[0], 10000 * w[0]], [-10000 * w[1], 10000 * w[1]], 'g') #green broken line in the plot
ax=plt.gca()
ax.set_xlim(-7.5,12)
ax.set_ylim(-7.5,10)


# Scikitlearn
tic = time.clock()

X = np.concatenate((x1, x2), axis = 0)
y = data[:, 2]
lda = LinearDiscriminantAnalysis()
lda.fit(X, y)
p_lda = lda.predict(X)
error_lda = np.sum(abs(p_lda - data[:,2]))
slope_sk = -lda.coef_[0][0]/lda.coef_[0][1]
thresh_sk = -lda.intercept_/lda.coef_[0][1]

toc = time.clock()
print('Elapsed time for scikit in seconds:', (toc - tic))

x = np.linspace(-4, 4, 50) 
y = thresh_sk + slope_sk * x
plt.plot(x, y, c='b') # blue solid line in the plot is the Scikit learn discriminant line


# Answer to Q1b
# Plot the FLD line
thresh = -0.005
slope = -w[0] / w[1]
y_int = -thresh / w[1]
x = np.linspace(-5,5,100)
y = slope*x + y_int
plt.plot(x, y, '--')
plt.show()


# Prediction and Precision
a=np.sign(np.dot(w, data[:, (0, 1)].T) +thresh)

# Answer to Q1 c
prediction = (np.sign(np.dot(w, data[:, (0, 1)].T) +thresh) + 1) / 2
Conf_mat = sklearn.metrics.confusion_matrix(test_labels, prediction, labels=None, sample_weight=None, normalize=None)

# Answer to Q1 d
error = np.sum(abs(prediction - data[:,2]))
percentage = error / len(data)

percentage

Conf_mat

"""# Question 2.

#### a) Using the multivariate data in the file spam.xlsx, determine the discriminant line found by Fishers Linear Discriminant. Using this line, determine the class of each of the data points in the training dataset, assuming that the threshold is 0.

#### b) Determine what percentage of the training data points are incorrectly classified by your classifier.

#### c) Perform the above analysis using only my implementation in Python and report the minimum error you could obtain by adjusting the threshold. For that minimum error value of the threshold, report the confusion matrix when classifying the training data.
"""

# Reading the data
tic = time.clock()
data = pd.read_excel('spam.xlsx')
data = pd.DataFrame.to_numpy(data)
toc = time.clock()
print('Elapsed time in seconds:', (toc - tic))

# loading the data to an array and separating classes

x1=data[0:500,0:57]    # class = 1
x2=data[500:1000, 0:57] # class = 0

test_labels = data[:,57]

# Calculate the mean for each group
u1 = np.mean(x1, axis = 0)
u2 = np.mean(x2, axis = 0)

# Remove the mean for each group
x1ur = x1 - u1
x2ur = x2 - u2

# Calculate the Cov for each group
cov1 = np.dot(x1ur.T, x1ur)
cov2 = np.dot(x2ur.T, x2ur)

# FLD
sw = cov1 + cov2
w = np.dot(np.linalg.inv(sw), (u1 - u2))

# Plt
plt.figure()
plt.scatter(x1[:, 0], x1[:, 1], c='r')
plt.scatter(x2[:, 0], x2[:, 1], c='b')
plt.plot([-10000 * w[0], 10000 * w[0]], [-10000 * w[1], 10000 * w[1]], 'g')
ax=plt.gca()
ax.set_xlim(-2,4)
ax.set_ylim(-2,6)
plt.show()

# Prediction and Precision
# thresh set to -0.001 for a better percentage in precision(0.101; 0.183 when thresh = 0)
thresh = -0.00247
slope = -w[0] / w[1]
y_int = -thresh / w[1]

# Answer to Q2 b
prediction = (np.sign(np.dot(w, data[:, 0:57].T) +thresh) + 1) / 2
error = np.sum(abs(data[:,57] - prediction))
percentage = error / len(data)

# Answer to Q2 c
Conf_mat = sklearn.metrics.confusion_matrix(test_labels, prediction, labels=None, sample_weight=None, normalize=None)

Conf_mat